{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNG15pfSq_tf"
   },
   "source": [
    "# Detección de anomalias usando Autoencoders\n",
    "\n",
    "Referencia [link](https://keras.io/examples/timeseries/timeseries_anomaly_detection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0S6STWYq_tg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Conv1DTranspose, Dropout, LSTM, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDDbD_N0q_th"
   },
   "source": [
    "## Datos\n",
    "\n",
    "Usaremos el conjunto de datos [Numenta Anomaly Benchmark (NAB)](https://www.kaggle.com/boltzmannbrain/nab). Este proporciona series de tiempo artificiales que contienen períodos anómalos de comportamiento etiquetados. Los datos están ordenados, tienen marcas de tiempo y son métricas de un solo valor.\n",
    "\n",
    "Usaremos el archivo `art_daily_small_noise.csv` para el entrenamiento y el archivo `art_daily_jumpsup.csv` para las pruebas. La simplicidad de este conjunto de datos nos permite demostrar la detección de anomalías de manera efectiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T22hIDE_q_th"
   },
   "outputs": [],
   "source": [
    "master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "df_small_noise_url = master_url_root + df_small_noise_url_suffix\n",
    "df_small_noise = pd.read_csv(\n",
    "    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")\n",
    "\n",
    "df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n",
    "df_daily_jumpsup = pd.read_csv(\n",
    "    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1ufnlGxq_th"
   },
   "outputs": [],
   "source": [
    "df_small_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_jumpsup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMHzoA-Vq_ti"
   },
   "source": [
    "## Visualización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9qTJdOQq_ti"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 6))\n",
    "df_small_noise.plot(legend=False, ax=ax[0], title=\"Series sin anomalías\")\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax[1], title=\"Series con anomalías\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9Y2JUxfq_tj"
   },
   "source": [
    "## Preprocesado\n",
    "\n",
    "Obtenemos los valores del archivo de datos de la serie de tiempo de entrenamiento y normalizamos los datos de `value`. Tenemos un `value` cada 5 minutos durante 14 días.\n",
    "\n",
    "* 24 * 60 / 5 = **288 pasos de tiempo por día**\n",
    "* 288 * 14 = **4032 puntos de datos** en total\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pGlVVorq_tj"
   },
   "outputs": [],
   "source": [
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "print(\"Número de ejemplos de entrenamiento:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHaW-dUXq_tj"
   },
   "source": [
    "## Creación de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OVn79t7q_tj"
   },
   "outputs": [],
   "source": [
    "TIME_STEPS = 288\n",
    "\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Shape entrenamiento: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DirCeLm7q_tj"
   },
   "source": [
    "## Construcción de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder = Sequential([\n",
    "    Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "    LSTM(64, activation=\"relu\", return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation=\"relu\", return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(16, activation=\"relu\", return_sequences=False),\n",
    "    RepeatVector(x_train.shape[1]),\n",
    "    LSTM(32, activation=\"relu\", return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation=\"relu\", return_sequences=True),\n",
    "    TimeDistributed(Dense(x_train.shape[2]))\n",
    "])\n",
    "\n",
    "lstm_autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=5e-3),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "lstm_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWDIZvLyq_tj"
   },
   "outputs": [],
   "source": [
    "conv_autoencoder = Sequential([\n",
    "    Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "    Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=7,\n",
    "        padding=\"same\",\n",
    "        strides=2,\n",
    "        activation=\"relu\",\n",
    "    ),\n",
    "    Dropout(rate=0.2),\n",
    "    Conv1D(\n",
    "        filters=16,\n",
    "        kernel_size=7,\n",
    "        padding=\"same\",\n",
    "        strides=2,\n",
    "        activation=\"relu\",\n",
    "    ),\n",
    "    Dropout(rate=0.2),\n",
    "    Conv1D(\n",
    "        filters=16,\n",
    "        kernel_size=7,\n",
    "        padding=\"same\",\n",
    "        strides=2,\n",
    "        activation=\"relu\",\n",
    "    ),\n",
    "    Dropout(rate=0.2),\n",
    "    Conv1DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=7,\n",
    "        padding=\"same\",\n",
    "        strides=2,\n",
    "        activation=\"relu\",\n",
    "    ),\n",
    "    Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "])\n",
    "\n",
    "conv_autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=5e-3),\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "conv_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJrr_6GOq_tj"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = lstm_autoencoder.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd-QYkM-q_tk"
   },
   "outputs": [],
   "source": [
    "history_2 = conv_autoencoder.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0qx3epTq_tk"
   },
   "outputs": [],
   "source": [
    "plt.plot(history_1.history[\"loss\"], label=\"LSTM - Pérdida de Entrenamiento\")\n",
    "plt.plot(history_1.history[\"val_loss\"], label=\"LSTM - Pérdida de Validación\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2.history[\"loss\"], label=\"Conv - Pérdida de Entrenamiento\")\n",
    "plt.plot(history_2.history[\"val_loss\"], label=\"Conv - Pérdida de Validación\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4eaFQF6q_tk"
   },
   "source": [
    "## Detectando anomalías\n",
    "\n",
    "Detectaremos anomalías determinando qué tan bien nuestro modelo puede reconstruir los datos de entrada.\n",
    "\n",
    "1. Calcular la pérdida MAE en las muestras de entrenamiento.\n",
    "2. Encontrar el valor máximo de la pérdida MAE. Este es el peor desempeño de nuestro modelo al intentar reconstruir una muestra. Usaremos este valor como el `umbral` para la detección de anomalías.\n",
    "3. Si la pérdida de reconstrucción para una muestra es mayor que este valor `umbral`, entonces podemos inferir que el modelo está observando un patrón con el que no está familiarizado. Etiquetaremos esta muestra como una `anomalía`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAh8xljSq_tk"
   },
   "outputs": [],
   "source": [
    "# Obtenemos el MAE.\n",
    "def calculate_mae(model, x_data, mode=\"entrenamiento\"):\n",
    "    x_data_pred = model.predict(x_data)\n",
    "    mae_loss = np.mean(np.abs(x_data_pred - x_data), axis=1)\n",
    "    plt.hist(mae_loss, bins=50)\n",
    "    plt.xlabel(\"MAE de {}\".format(mode)))\n",
    "    plt.ylabel(\"Número de muestras\")\n",
    "    plt.show()\n",
    "    threshold = np.max(mae_loss)\n",
    "    return x_data_pred, threshold, mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_pred, lstm_threshold, _ = calculate_mae(lstm_autoencoder, x_train)\n",
    "conv_train_pred, conv_threshold, _ = calculate_mae(conv_autoencoder, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH92kxhMq_tk"
   },
   "source": [
    "### Comparar reconstrucción\n",
    "\n",
    "Veamos cómo nuestro modelo ha reconstruido la primera muestra.\n",
    "Esta corresponde a los **288 pasos de tiempo del día 1** de nuestro conjunto de entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD9OH1e8q_tk"
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train[0])\n",
    "plt.plot(lstm_train_pred[0])\n",
    "plt.title(\"Reconstrucción con autoencoder LSTM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[0])\n",
    "plt.plot(conv_train_pred[0])\n",
    "plt.title(\"Reconstrucción con autoencoder Conv\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl4U690nq_tk"
   },
   "source": [
    "### Preparación de los datos test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWqijKK1q_tl"
   },
   "outputs": [],
   "source": [
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Shape testeo: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_pred, _, lstm_test_mae = calculate_mae(lstm_autoencoder, x_test, mode=\"testeo\")\n",
    "lstm_test_mae = lstm_test_mae.reshape((-1))\n",
    "anomalies = lstm_test_mae > lstm_threshold\n",
    "plt.hist(lstm_test_mae, bins=50)\n",
    "plt.xlabel(\"MAE de testeo\")\n",
    "plt.ylabel(\"Número de muestras\")\n",
    "plt.show()\n",
    "print(\"Número de muestras anómalas: \", np.sum(anomalies))\n",
    "print(\"Índices de las muestras anómalas: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test_pred, _, conv_test_mae = calculate_mae(conv_autoencoder, x_test, mode=\"testeo\")\n",
    "conv_test_mae = conv_test_mae.reshape((-1))\n",
    "anomalies = conv_test_mae > conv_threshold\n",
    "plt.hist(conv_test_mae, bins=50)\n",
    "plt.xlabel(\"MAE de testeo\")\n",
    "plt.ylabel(\"Número de muestras\")\n",
    "plt.show()\n",
    "print(\"Número de muestras anómalas: \", np.sum(anomalies))\n",
    "print(\"Índices de las muestras anómalas: \", np.where(anomalies))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "timeseries_anomaly_detection",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (mdl2501)",
   "language": "python",
   "name": ".mdl2501"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
